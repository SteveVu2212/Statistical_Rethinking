---
title: "UCBadmit"
output: github_document
---

# Instalation

```{r}
rm(list=ls())
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(MASS)
library(rstan)
library(shape)
library(tidyr)
require(visdat)
library(ggplot2)
library(dagitty)
library(gtools)
library(ellipse)
library(tidyverse)
library(rethinking)
```

```{r}
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

# Load and process data

```{r}
data("UCBadmit")
d <- UCBadmit
```

```{r}
str(d)
```


# 1. Models with aggregated data
## Inital statistical models

$$
A_{i} \sim Binomial(N_{i},p_{i})\\
logit(p_{i}) = \alpha_{GID[i]}\\
\alpha_{GID[i]} \sim 
$$

## Define prior

We're going to test a flat and tight prior of alpha, and plot the prior predictive check

```{r}
set.seed(1)
N <- 100
dens(inv_logit(rnorm(N,mean=0, sd=10)), adj=0.1, col=rangi2)
text(0.8,8,"a ~ Normal(0,10)", col=rangi2)
dens(inv_logit(rnorm(N,mean=0, sd=1.5)), adj=0.1, add=TRUE)
text(0.4,4,"a ~ Normal(0,1.5)")
```

Apparently, a flat prior on the logit space piles up the probability on zero and one. We will use the Normal(0,1.5) prior for alpha

## Updated statistical models

$$
A_{i} \sim Binomial(N_{i},p_{i})\\
logit(p_{i}) = \alpha_{GID[i]}\\
\alpha_{GID[i]} \sim Normal(0,1.5)
$$

```{r}
dlist.1 <- list(
  admit = d$admit,
  applications = d$applications,
  gid = ifelse(d$applicant.gender=="male",1,2),
  N = nrow(d)
)
```


```{r}
code_m1.1 <- "
data{
  int N;
  int admit[N];
  int applications[N];
  int gid[N];
}
parameters{
  vector[2] a;
}
model{
  vector[N] p;
  a ~ normal(0,1.5);
  for(i in 1:N){
    p[i] = a[gid[i]];
    p[i] = inv_logit(p[i]);
  }
  admit ~ binomial(applications, p);
}
generated quantities{
  vector[N] log_lik;
  vector[N] p;
  for(i in 1:N){
    p[i] = a[gid[i]];
    p[i] = inv_logit(p[i]);
  }
  for(i in 1:N){
    log_lik[i] = binomial_lpmf(admit[i] | applications[i], p[i]);
  }
}
"
```


```{r}
m.1.1 <- stan(model_code=code_m1.1, data=dlist.1, chains=4, cores=4)
precis(m.1.1,2, pars=c("a"))
```

## Posterior predictive check

We're interested in the difference in applications between gender. So, we need to compute the contrast

```{r}
post.1.1 <- extract.samples(m.1.1)
diff.a <- post.1.1$a[,1] - post.1.1$a[,2]
diff.p <- inv_logit(post.1.1$a[,1]) - inv_logit(post.1.1$a[,2])
precis(list(diff.a=diff.a, diff.p=diff.p))
```

The model estimates a higher posterior for male applicants than of female applicants

```{r}
mu_mean <- rep(0,12)
mu_pi_lower <- rep(0,12)
mu_pi_upper <- rep(0,12)
for(i in 1:12){
  app <- d$applications[i]
  gid <- ifelse(i%%2==0,2,1)
  sample <- rbinom(n=1e4, size=app, prob=inv_logit(post.1.1$a[,ifelse(i%%2==0,2,1)]))
  admit <- sample/app
  mu_mean[i] <- mean(admit)
  mu_pi_lower[i] <- PI(admit, prob=0.89)[1]
  mu_pi_upper[i] <- PI(admit, prob=0.89)[2]
}
```

```{r, fig.height=4, fig.width=4}
plot(NULL, xlim=c(1,12),ylim=c(0,1),
     xlab="admit", ylab="case", xaxt="n", yaxt="n")
axis(side=2,at=seq(0,1,by=0.2), labels=c(0.0,0.2,0.4,0.6,0.8,1.0))
axis(side=1,at=seq(1,12,by=1), labels=seq(1,12,by=1))

points(x=1:12, y=mu_mean, pch=10)
for(i in 1:12){points(x=rep(i,2), y=c(mu_pi_lower[i],mu_pi_upper[i]), pch=3)}

points(x=1:12,y=d$admit/d$applications, pch=19, col=rangi2)
for(i in 1:6){lines(x=c(2*i-1,2*i),y=c(d$admit[2*i-1]/d$applications[2*i-1],d$admit[2*i]/d$applications[2*i]), col=rangi2)}

for(i in 1:6){text(x=2*i-0.5,y=mean(c(d$admit[2*i-1]/d$applications[2*i-1],d$admit[2*i]/d$applications[2*i]))+0.04, labels = d$dept[2*i])}
```
It is obvious that the model badly fits the data and the posterior prediction is off. That arises a motivating question of the average difference in probability of admission between women and men within department

## Updated statistical models

$$
A_{i} \sim Binomial(N_{i},p_{i})\\
logit(p_{i}) = \alpha_{GID[i]} + \delta_{DEPT[i]}\\
\alpha_{GID[i]} \sim Normal(0,1.5)\\
\delta_{DEPT[i]} \sim Normal(0,1.5)
$$
```{r}
dlist.1$dept_id <- rep(1:6,each=2)
```

```{r}
code_m1.2 <- "
data{
  int N;
  int admit[N];
  int applications[N];
  int gid[N];
  int dept_id[N];
}
parameters{
  vector[2] a;
  vector[6] b;
}
model{
  vector[N] p;
  b ~ normal(0,1.5);
  a ~ normal(0,1.5);
  for(i in 1:N){
    p[i] = a[gid[i]] + b[dept_id[i]];
    p[i] = inv_logit(p[i]);
  }
  admit ~ binomial(applications, p);
}
generated quantities{
  vector[N] log_lik;
  vector[N] p;
  for(i in 1:N){
    p[i] = a[gid[i]] + b[dept_id[i]];
    p[i] = inv_logit(p[i]);
  }
  for(i in 1:N){
    log_lik[i] = binomial_lpmf(admit[i] | applications[i], p[i]);
  }
}
"
```

```{r}
m.1.2 <- stan(model_code = code_m1.2, data = dlist.1, chains = 4, cores = 4)
precis(m.1.2,2, pars=c("a", "b"))
```

```{r}
post <- extract.samples(m.1.2)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )
```

```{r}
dat <- list(gid=rep(1:2, times=6), dept_id=rep(1:6, each=2))

admit_post <- matrix(0, nrow=nrow(post$a), ncol=12)
for(i in 1:12){admit_post[,i] = inv_logit(post$a[,dat$gid[i]] + post$b[,dat$dept_id[i]])}

# admit_post <- link(m1.2, data=dat)
admit_mu <- apply(admit_post, 2, mean)
admit_pi <- apply(admit_post, 2,PI, prob=0.89)
```


```{r}
plot(NULL, xlim=c(1,12),ylim=c(0,1),
     xlab="admit", ylab="case", xaxt="n", yaxt="n")
axis(side=2,at=seq(0,1,by=0.2), labels=c(0.0,0.2,0.4,0.6,0.8,1.0))
axis(side=1,at=seq(1,12,by=1), labels=seq(1,12,by=1))

points(x=1:12, y=admit_mu, pch=10)
for(i in 1:12){points(x=rep(i,2), y=admit_pi[,i], pch=3)}

points(x=1:12,y=d$admit/d$applications, pch=19, col=rangi2)
for(i in 1:6){lines(x=c(2*i-1,2*i),y=c(d$admit[2*i-1]/d$applications[2*i-1],d$admit[2*i]/d$applications[2*i]), col=rangi2)}

for(i in 1:6){text(x=2*i-0.5,y=mean(c(d$admit[2*i-1]/d$applications[2*i-1],d$admit[2*i]/d$applications[2*i]))+0.04, labels = d$dept[2*i])}
```

The model fits the data very well. It proves that department is a mediator. We can draw a causal diagram to illustrate the relationship

```{r}
dag1.1 <- dagitty("dag{G->A
                  G->D->A}")
coordinates(dag1.1) <- list(x=c(G=0,D=1,A=2),y=c(G=0,D=-1,A=0))
drawdag(dag1.1)
```

Although the model performs well, it is over-parameterized as we don't actually need either a[1] or a[2]

While including the mediator cures the model, another way is to construct a beta-binomial model which deals with over-dispersed cases

#2. Beta-binomial models

The ultimate goal is to estimate the distribution of probabilities of success instead of a single probability of success by using Beta distribution

Note that a beta distribution has two parameters, an average probability and a shape parameter

## Statistical model

$$
A_{i} \sim BetaBinomial(N_{i},\bar p_{i}, \theta)\\
logit(\bar p_{i}) = \alpha_{GID[i]}\\
\alpha_{GID[i]} \sim Normal(0,1.5)\\
\theta = \phi + 2\\
\phi \sim Exponential(1)
$$

```{r}
code_m2.1 <- "
data{
  int N;
  int admit[N];
  int applications[N];
  int gid[N];
}
parameters{
  vector[2] a;
  real<lower=0> phi;
}
transformed parameters{
  real theta;
  theta = phi + 2;
}
model{
  vector[N] p_bar;
  phi ~ exponential(1);
  a ~ normal(0, 1.5);
  for(i in 1:N){
    p_bar[i] = a[gid[i]];
    p_bar[i] = inv_logit(p_bar[i]);
  }
  admit ~ beta_binomial(applications, p_bar*theta, (1-p_bar)*theta);
}
generated quantities{
  vector[N] log_lik;
  vector[N] p_bar;
  for(i in 1:N){
    p_bar[i] = a[gid[i]];
    p_bar[i] = inv_logit(p_bar[i]);
  }
  for(i in 1:N){
    log_lik[i] = beta_binomial_lpmf(admit[i] | applications[i], p_bar*theta, (1-p_bar)*theta);
  }
}
"
```


```{r}
m.2.1 <- stan(model_code=code_m2.1, data=dlist.1, chains=4, cores=4)
precis(m.2.1,2, pars=c("a", "phi", "theta"))
```

```{r}
post <- extract.samples(m.2.1)
post$da <- post$a[,1] - post$a[,2]
precis(post$da)
```

While the contrast shows less evidence of a difference between male and female admission rates, it is clear that the sampling is poor performance with low number of efficient samples and high value of R square.

```{r}
gid <- 2
# draw posterior mean beta distribution
curve( dbeta2(x,mean(logistic(post$a[,gid])),mean(post$theta)) , from=0 , to=1 ,
    ylab="Density" , xlab="probability admit", ylim=c(0,3) , lwd=2 )
# draw 50 beta distributions sampled from posterior
for ( i in 1:50 ) {
    p <- logistic( post$a[i,gid] )
    theta <- post$theta[i]
    curve( dbeta2(x,p,theta) , add=TRUE , col=col.alpha("black",0.2) )
}
mtext( "distribution of female admission rates" )
```

```{r}
mu_mean <- rep(0,12)
mu_pi_lower <- rep(0,12)
mu_pi_upper <- rep(0,12)
for(i in 1:12){
  app <- d$applications[i]
  gid <- ifelse(i%%2==0,2,1)
  sample <- rbinom(n=1e4, size=app, prob=inv_logit(post$a[,ifelse(i%%2==0,2,1)]))
  admit <- sample/app
  mu_mean[i] <- mean(admit)
  mu_pi_lower[i] <- PI(admit, prob=0.89)[1]
  mu_pi_upper[i] <- PI(admit, prob=0.89)[2]
}
```

```{r, fig.height=4, fig.width=4}
plot(NULL, xlim=c(1,12),ylim=c(0,1),
     xlab="admit", ylab="case", xaxt="n", yaxt="n")
axis(side=2,at=seq(0,1,by=0.2), labels=c(0.0,0.2,0.4,0.6,0.8,1.0))
axis(side=1,at=seq(1,12,by=1), labels=seq(1,12,by=1))

points(x=1:12, y=mu_mean, pch=10)
for(i in 1:12){points(x=rep(i,2), y=c(mu_pi_lower[i],mu_pi_upper[i]), pch=3)}

points(x=1:12,y=d$admit/d$applications, pch=19, col=rangi2)
for(i in 1:6){lines(x=c(2*i-1,2*i),y=c(d$admit[2*i-1]/d$applications[2*i-1],d$admit[2*i]/d$applications[2*i]), col=rangi2)}

for(i in 1:6){text(x=2*i-0.5,y=mean(c(d$admit[2*i-1]/d$applications[2*i-1],d$admit[2*i]/d$applications[2*i]))+0.04, labels = d$dept[2*i])}
```

Although the model does not fit the data well, the point is that it does see heterogeneity across rows and uses the beta distribution to estimate and anticipate that heterogeneity

#3. Multilevel models
## Models with varying intercepts

```{r}
dlist.2 <- list(
  male = ifelse(d$applicant.gender=="male",1,2),
  dept = rep(1:6, each=2),
  applications = d$applications,
  admit = d$admit,
  N = nrow(d)
)
```


```{r}
code_m3.1 <- "
data{
  int N;
  int male[N];
  int dept[N];
  int applications[N];
  int admit[N];
}
parameters{
  vector[6] a;
  real a_bar;
  real b;
  real<lower=0> sigma;
}
model{
  vector[N] p;
  sigma ~ normal(0,1);
  b ~ normal(0,1);
  a_bar ~ normal(0,4);
  a ~ normal(a_bar, sigma);
  for(i in 1:N){
    p[i] = a[dept[i]] + b*male[i];
    p[i] = inv_logit(p[i]);
  }
  admit ~ binomial(applications, p);
}
generated quantities{
  vector[N] log_lik;
  vector[N] p;
  for(i in 1:N){
    p[i] = a[dept[i]] + b*male[i];
    p[i] = inv_logit(p[i]);
  }
  for(i in 1:N){
    log_lik[i] = binomial_lpmf(admit[i] | applications[i], p[i]);
  }
}
"
```


```{r}
m.3.1 <- stan(model_code=code_m3.1, data=dlist.2, chains=4, cores=4)
precis(m.3.1,2, pars=c("a", "a_bar", "b", "sigma"))
```

## Anagolous models with varying slopes

```{r}
code_m3.2 <- "
data{
  int N;
  int dept[N];
  int male[N];
  int admit[N];
  int applications[N];
}
parameters{
  corr_matrix[2] Rho;
  vector<lower=0>[2] sigma;
  real a_bar;
  real b_bar;
  vector[6] a;
  vector[6] b;
}
model{
  vector[N] p;
  sigma ~ normal(0,1);
  Rho ~ lkj_corr(2);
  a_bar ~ normal(0,4);
  b_bar ~ normal(0,1);
  {
  vector[2] MU;
  vector[2] YY[6];
  
  MU = [a_bar, b_bar]';
  for(i in 1:6){YY[i] = [a[i], b[i]]';}
  YY ~ multi_normal(MU, quad_form_diag(Rho, sigma));
  }
  for(i in 1:N){
    p[i] = a[dept[i]] + b[dept[i]]*male[i];
    p[i] = inv_logit(p[i]);
  }
  admit ~ binomial(applications, p);
}
generated quantities{
  vector[N] log_lik;
  vector[N] p;
  for(i in 1:N){
    p[i] = a[dept[i]] + b[dept[i]]*male[i];
    p[i] = inv_logit(p[i]);
  }
  for(i in 1:N){
    log_lik[i] = binomial_lpmf(admit[i] | applications[i], p[i]);
  }
}
"
```


```{r}
m.3.2 <- stan(model_code = code_m3.2, data = dlist.2, chains = 4, cores = 4)
precis(m.3.2,2, pars=c("a_bar", "b_bar", "sigma"))
```


```{r}
code_m3.3 <- "
data{
  int N;
  int dept[N];
  int male[N];
  int admit[N];
  int applications[N];
}
parameters{
  corr_matrix[2] Rho;
  vector<lower=0>[2] sigma;
  real a_bar;
  real b_bar;
  vector[2] v[6];
}
model{
  vector[N] p;
  sigma ~ normal(0,1);
  Rho ~ lkj_corr(2);
  a_bar ~ normal(0,4);
  b_bar ~ normal(0,1);
  {
  vector[2] MU;

  MU = [a_bar, b_bar]';
  v ~ multi_normal(MU, quad_form_diag(Rho, sigma));
  }
  for(i in 1:N){
    p[i] = v[dept[i],1] + v[dept[i],2]*male[i];
    p[i] = inv_logit(p[i]);
  }
  admit ~ binomial(applications, p);
}
generated quantities{
  vector[N] log_lik;
  vector[N] p;
  for(i in 1:N){
    p[i] = v[dept[i],1] + v[dept[i],2]*male[i];
    p[i] = inv_logit(p[i]);
  }
  for(i in 1:N){
    log_lik[i] = binomial_lpmf(admit[i] | applications[i], p[i]);
  }
}
"
```


```{r}
m.3.3 <- stan(model_code=code_m3.3, data=dlist.2, chains=4, cores=4)
precis(m.3.3,2, pars=c("a_bar", "b_bar", "sigma"))
```

```{r}
code_m3.4 <- "
data{
  int N;
  int dept[N];
  int male[N];
  int admit[N];
  int applications[N];
}
parameters{
  corr_matrix[2] Rho;
  vector<lower=0>[2] sigma;
  vector[2] v[6];
  vector[2] v_mu;
}
model{
  vector[N] p;
  sigma ~ normal(0,1);
  Rho ~ lkj_corr(2);
  v_mu ~ normal(0,1);
  v ~ multi_normal(v_mu, quad_form_diag(Rho, sigma));
  for(i in 1:N){
    p[i] = v[dept[i],1] + v[dept[i],2]*male[i];
    p[i] = inv_logit(p[i]);
  }
  admit ~ binomial(applications, p);
}
generated quantities{
  vector[N] log_lik;
  vector[N] p;
  for(i in 1:N){
    p[i] = v[dept[i],1] + v[dept[i],2]*male[i];
    p[i] = inv_logit(p[i]);
  }
  for(i in 1:N){
    log_lik[i] = binomial_lpmf(admit[i] | applications[i], p[i]);
  }
}
"
```


```{r}
m.3.4 <- stan(model_code=code_m3.4, data=dlist.2, chains=4, cores=4)
precis(m.3.4,2, pars=c("sigma", "v_mu"))
```

```{r}
code_m3.5 <- "
data{
  int N;
  int admit[N];
  int applications[N];
  int male[N];
  int dept[N];
}
parameters{
  matrix[2,6] z;
  vector[2] v_mu;
  vector<lower=0>[2] sigma;
  cholesky_factor_corr[2] L_Rho;
}
model{
  vector[N] p;
  matrix[6,2] v;
  
  L_Rho ~ lkj_corr_cholesky(2);
  sigma ~ normal(0,1);
  v_mu[1] ~ normal(0,4);
  v_mu[2] ~ normal(0,1);
  to_vector(z) ~ normal(0,1);
  
  v = (diag_pre_multiply(sigma, L_Rho)*z)';
  for(i in 1:N){
    p[i] = v_mu[1] + v[dept[i],1] + (v_mu[2] + v[dept[i],2])*male[i];
    p[i] = inv_logit(p[i]);
  }
  
  admit ~ binomial(applications, p);
}
generated quantities{
  vector[N] log_lik;
  vector[N] p;
  matrix[6,2] v;
  v = (diag_pre_multiply(sigma, L_Rho)*z)';
  for(i in 1:N){
    p[i] = v_mu[1] + v[dept[i],1] + (v_mu[2] + v[dept[i],2])*male[i];
    p[i] = inv_logit(p[i]);
  }
  for(i in 1:N){
    log_lik[i] = binomial_lpmf(admit[i] | applications[i], p[i]);
  }
}
"
```


```{r}
m.3.5 <- stan(model_code=code_m3.5, data=dlist.2, chains=4, cores=4)
precis(m.3.5,3, pars=c("v_mu", "sigma", "L_Rho"))
```

```{r}
compare(m.1.2, m.3.1, m.3.2, m.3.3, m.3.4, m.3.5)
```


```{r}
```