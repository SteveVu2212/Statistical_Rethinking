---
title: "Exercise-C8"
output: github_document
---

```{r}
rm(list=ls())
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(rethinking)
library(ggplot2)
library(dplyr)
library(gridExtra)
```


# 8E1
(1) Water and yeast rise bread dough
(2) Education and parent's education level leads to higher income
(3) Gasoline and machine make a car go

# 8E2 + 8E3

(2) Cylinders and fuel injector make a car go faster
(3) Parents and friends influence people's political beliefs
(4) Social and manipulative appendages make animal species intelligent

# 8M1

Hot temperature dries out plants regardless of water and shade. As results, plants developed no blooms under a hot temperature condition

# 8M2

$$
B_{i} \sim Normal(mu_{i},sigma)\\
\mu_{i} = \alpha + \beta_{WST}*W_{i}*S_{i}*T_{i}
$$

# 8M3

The size of raven population depends on both available sources of food and the wolf population.

$$
R \sim Normal(mu, sigma)\\
\mu = a + bW*W + bF*F + bWF*W*F
$$

The biological interaction can be linear on one case and non-linear in other cases. In cases of special living environments where wolves are the only carnivorous and hunting animals, ravens are hugely dependent on wolf population. However, ravens can search for open carcasses of prey from other hunters if they are available.

# 8M4

```{r}
data("tulips")
d <- tulips
tail(d)
```

```{r}
d$bed <- as.integer(d$bed)
```


```{r}
d$blooms_std <- d$blooms / max(d$blooms)
d$water_cent <- d$water - mean(d$water)
d$shade_cent <- d$shade - mean(d$shade)
```


```{r}
m8.5 <- quap(
  alist(
    blooms_std ~ dnorm(mu, sigma),
    mu <- a + bw*water_cent + bs*shade_cent + bws*water_cent*shade_cent,
    a ~ dnorm(0.5,0.25),
    bw ~ dnorm(0.5,0.25),
    bs ~ dnorm(-0.5,0.25),
    bws ~ dnorm(-0.5,0.25),
    sigma ~ dexp(1)
  ),
  data=d
)
```

```{r}
bw <- rnorm(100,0.5,0.25)
sum(bw > 0)/100
```

```{r}
precis(m8.5)
```


```{r}
set.seed(2021)
prior <- extract.prior(m8.5)
```


```{r, fig.height=3, fig.width=7}
par(mfrow=c(1,3))
for(s in -1:1){
  idx <- which(d$shade_cent==s)
  plot(d$water_cent[idx],d$blooms_std[idx],
       xlim=c(-1,1),ylim=c(0,1),
       xlab="water",ylab="blooms",pch=16,col=rangi2,
       cex.lab=1.5)
  mu <- link(m8.5,post=prior, data=data.frame(shade_cent=s, water_cent=-1:1))
  for(i in 1:20){
  lines(-1:1, mu[i,], col=col.alpha("black",0.3))}
}
```

Using constrained priors makes the lines in left-side and middle charts to have positive slope. Increases in water levels help plants to develop blooms faster. The last chart illustrates an unclear trend as shade cent has critical impacts on the prior prediction.

# 8H1 + 8H2

```{r}
h8.1 <- quap(
  alist(
    blooms_std ~ dnorm(mu, sigma),
    mu <- a[bed] + bw*water_cent + bs*shade_cent + bws*water_cent*shade_cent,
    a[bed] ~ dnorm(0.5,0.25),
    bw ~ dnorm(0.5,0.25),
    bs ~ dnorm(-0.5,0.25),
    bws ~ dnorm(-0.5,0.25),
    sigma ~ dexp(1)
  ),
  data=d
)
```


```{r}
precis(h8.1, depth = 2)
```
```{r}
compare(m8.5, h8.1, func = WAIC)
```
```{r}
compare(m8.5, h8.1, func=PSIS)
```

WAIC and PSIS produces two opposite outcomes of model comparisons.

While h8.1 has more parameters than m8.5, the additional ones created by adding a categorical variable of bed make no significant compensatory increases in predictive power measured by WAIC and PSIS.

# 8H3

```{r}
data("rugged")
d <- rugged
d$log_gdp <- log(d$rgdppc_2000)
dd <- d[complete.cases(d$rgdppc_2000),]
dd$log_gdp_std <- dd$log_gdp/mean(dd$log_gdp)
dd$rugged_std <- dd$rugged/max(dd$rugged)
dd$cid <- ifelse(dd$cont_africa==1,1,2)
```


```{r}
m8.3 <- quap(
  alist(
    log_gdp_std ~ dnorm(mu, sigma),
    mu <- a[cid] + b[cid]*(rugged_std - 0.215),
    a[cid] ~ dnorm(1,0.1),
    b[cid] ~ dnorm(0,0.3),
    sigma ~ dexp(1)
  ),
  data = dd
)
```


```{r}
PSIS.m8.3 <- PSIS(m8.3, pointwise = TRUE)
WAIC.m8.3 <- WAIC(m8.3, pointwise = TRUE)
```


```{r}
plot(WAIC.m8.3$penalty, PSIS.m8.3$k,
     xlab="PSIS k value", ylab="WAIC penalty")
```


```{r}
h8.3 <- quap(
  alist(
    log_gdp_std ~ dstudent(2,mu, sigma),
    mu <- a[cid] + b[cid]*(rugged_std - 0.215),
    a[cid] ~ dnorm(1,0.1),
    b[cid] ~ dnorm(0,0.3),
    sigma ~ dexp(1)
  ),
  data = dd
)
```


```{r}
PSIS.h8.3 <- PSIS(h8.3, pointwise = TRUE)
WAIC.h8.3 <- WAIC(h8.3, pointwise = TRUE)
```


```{r}
plot(WAIC.h8.3$penalty, PSIS.h8.3$k,
     xlab="PSIS k value", ylab="WAIC penalty")
```

A robust regression handles the phenominon of influential data points

# 8H4

```{r}
data("nettle")
d <- nettle
tail(d)
```

```{r}
dim(d)
```


```{r}
d$lang.per.cap <- d$num.lang/d$k.pop
d$lang.div <- log(d$lang.per.cap)
d$log.area <- log(d$area)
```

(a): The hypothesis is that area and average length of growing season have opposite impacts on language diversity. A large area natures new languages while fruitful seasons require less cooperation to survive.

```{r}
h8.4a <- quap(
  alist(
    lang.div ~ dnorm(mu, sigma),
    mu <- a + bM*mean.growing.season + bA*log.area,
    a ~ dnorm(-3,0.25),
    bM ~ dnorm(0,0.5),
    bA ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ),
  data=d
)
```


```{r}
set.seed(7)
prior <- extract.prior(h8.4a)

mu <- link(h8.4a, post=prior)

plot(d$mean.growing.season, d$lang.div,
     xlab="Average length of growing season",
     ylab="Language diversity")
for(i in 1:20){
  lines(d$mean.growing.season, mu[i,],col=col.alpha("black",0.1))
}
```


```{r}
precis(h8.4a)
```


```{r}
h8.4b <- quap(
  alist(
    lang.div ~ dnorm(mu, sigma),
    mu <- a + bS*sd.growing.season + bA*log.area,
    a ~ dnorm(-3,0.25),
    bS ~ dnorm(0,0.5),
    bA ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ),
  data=d
)
```


```{r}
set.seed(8)
prior <- extract.prior(h8.4b)

mu <- link(h8.4b, post=prior)

plot(d$mean.growing.season, d$lang.div,
     xlab="Average length of growing season",
     ylab="Language diversity")
for(i in 1:20){
  lines(d$mean.growing.season, mu[i,],col=col.alpha("black",0.1))
}
```


```{r}
precis(h8.4b)
```


```{r}
h8.4c <- quap(
  alist(
    lang.div ~ dnorm(mu, sigma),
    mu <- a + bM*mean.growing.season + bS*sd.growing.season + bAS*mean.growing.season*sd.growing.season + bA*log.area,
    a ~ dnorm(-3,0.25),
    bS ~ dnorm(0,1),
    bA ~ dnorm(0,0.5),
    bM ~ dnorm(0,0.5),
    bAS ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ),
  data=d
)
```


```{r}
precis(h8.4c)
```


```{r}
compare(h8.4a, h8.4b, h8.4c, func=WAIC)
```
The model comparison points out that the average length of growing seasons and area are the main factor to language diversity. The impact of standard deviation of length of growing season disappears in the interaction model.

# 8H5

```{r}
data("Wines2012")
d <- Wines2012
```


```{r}
d$std.score <- standardize(d$score)
d$wine.idx <- as.integer(d$wine)
d$judge.idx <- as.integer(d$judge)
```

```{r}
tail(d)
```

```{r, fig.height=2,fig.width=3.5}
p1 <- ggplot(data=d, aes(x=judge.idx,y=score)) + geom_point(col=col.alpha(rangi2,0.5))+
  scale_x_discrete(limits=seq(min(d$judge.idx),max(d$judge.idx)))
p2 <- ggplot(data=d, aes(x=wine.idx,y=score)) + geom_point(col=col.alpha(rangi2,0.5))+
  scale_x_discrete(limits=seq(min(d$wine.idx),max(d$wine.idx)))+
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank())
grid.arrange(p1,p2,nrow=1)
```
The influence of type of wines on the score is vague while some judgers are more generous about scoring and the others are strict.


```{r}
d1 <- d %>%
  select(std.score, wine.idx) %>%
  group_by(wine.idx) %>%
  summarise(mean.score=mean(std.score))

d2 <- d %>%
  select(std.score, judge.idx) %>%
  group_by(judge.idx) %>%
  summarise(mean.score=mean(std.score))
```

```{r}
d1 %>% summarise(sd = sd(mean.score))
```

```{r}
which.max(d1$mean.score); which.min(d1$mean.score)
```

Wine #4 has the highest score while wine #8 has the lowest score

```{r}
d2 %>% summarise(sd = sd(mean.score))
```

```{r}
which.max(d2$mean.score); which.min(d2$mean.score)
```

Judge #5 is the most generous while Judge #8 is strict

```{r}
h8.5 <- quap(
  alist(
    std.score ~ dnorm(mu, sigma),
    mu <- a[wine.idx] + b[judge.idx],
    a[wine.idx] ~ dnorm(0,0.33),
    b[judge.idx] ~ dnorm(0,0.55),
    sigma ~ dexp(1)
  ),
  data=d
)
```

```{r}
precis(h8.5,depth = 2)
```

The posterior reconfirms the pattern recognized from the charts as the coefficients of wine categorical variables show tiny impacts of types of wine on scores.

# 8H6

```{r}
tail(d)
```


```{r}
d$judge.amer <- d$judge.amer + 1
d$wine.amer <- d$wine.amer +1
d$flight.idx <- as.integer(d$flight)
```


```{r}
d %>%
  select(std.score, judge.amer) %>%
  group_by(judge.amer) %>%
  summarise(mean.score=mean(std.score)) %>%
  summarise(sd = sd(mean.score), mean = mean(mean.score))
```


```{r}
d %>%
  select(std.score, wine.amer) %>%
  group_by(wine.amer) %>%
  summarise(mean.score=mean(std.score)) %>%
  summarise(sd = sd(mean.score), mean = mean(mean.score))
```


```{r}
d %>%
  select(std.score, flight.idx) %>%
  group_by(flight.idx) %>%
  summarise(mean.score=mean(std.score)) %>%
  summarise(sd = sd(mean.score), mean = mean(mean.score))
```


```{r}
h8.6 <- quap(
  alist(
    std.score ~ dnorm(mu, sigma),
    mu <- a[judge.amer] + b[wine.amer] + c[flight.idx],
    a[judge.amer] ~ dnorm(0,0.2),
    b[wine.amer] ~ dnorm(0,0.2),
    c[flight.idx] ~ dnorm(0,0.2),
    sigma ~ dexp(1)
  ),
  data = d
)
```


```{r}
precis(h8.6, depth=2)
```
There is no clear difference between scores of American and French wine and/or judge

# 8H7

```{r}
head(d)
```


```{r}
d$interaction1 <- ifelse(d$wine.amer == 2 | d$flight == 2,2,1)
d$interaction2 <- ifelse(d$wine.amer == 2 | d$judge.amer == 2,2,1)
d$interaction3 <- ifelse(d$flight == 2 | d$judge.amer == 2,2,1)
```


```{r}
h8.7 <- quap(
  alist(
    std.score ~ dnorm(mu, sigma),
    mu <- a[interaction1] + b[interaction2] + c[interaction3],
    a[interaction1] ~ dnorm(0,0.1),
    b[interaction2] ~ dnorm(0,0.1),
    c[interaction3] ~ dnorm(0,0.1),
    sigma ~ dexp(1)
  ),
  data = d
)
```

```{r}
precis(h8.7, depth = 2)
```
The model's outcomes are compatible with the others as there is no statistical difference in scores between American and French wines and judges.

```{r}
compare(h8.5, h8.6, h8.7, func = WAIC)
```
The model comparison shows that the first model has the highest predictive power as we are able to extract information about judges' scores. The interaction terms in the third model are very weak 

```{r}
```


```{r}
```


```{r}
```

