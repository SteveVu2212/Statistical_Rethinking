---
title: "Milk"
output: github_document
---
# Instalation

```{r}
rm(list=ls())
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
library(MASS)
library(rstan)
library(shape)
library(tidyr)
require(visdat)
library(ggplot2)
library(dagitty)
library(gtools)
library(ellipse)
library(tidyverse)
library(rethinking)
```

```{r}
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

# Load data

The research question is which factors causaly impact the energy content of milk, measured by kilocarlories.

```{r}
data(milk)
d <- milk
str(d)
```

```{r}
d$K <- standardize(d$kcal.per.g)
d$N <- standardize(d$neocortex.perc)
d$M <- standardize(log(d$mass))
```

There are missing values in the data. Firstly, we will do a complete case analysis. Eventually, we will analyze the way to impute these missing values and extract information from them.

```{r}
vis_dat(d)
```

# 1.Mask relationship

```{r}
dcc <- d[complete.cases(d$K,d$N,d$M),]
```

## Inital statistical models

$$
K \sim Normal(mu, sigma)\\
mu = \alpha + \beta_{N}N\\
\alpha \sim Normal(0,1)\\
\beta_{N} \sim Normal(0,1)\\
\sigma \sim Exponential(1)
$$

## Prior predictive check

```{r}
set.seed(1)
N <- 100
a <- rnorm(N,0,1)
bN <- rnorm(N,0,1)
sigma <- rexp(N,1)
```


```{r}
plot(NULL, xlim=range(dcc$N),ylim=c(-2,2),
     xlab="N (std)", ylab="K (std)")
for (i in 1:50){
  curve(a[i]+bN[i]*x, from=min(dcc$N), to=max(dcc$N), add=TRUE,
        col=col.alpha(rangi2))
}
```
```{r}
N.seq <- seq(from=-2, to=2, length.out=30)
sim.K <- sapply(N.seq,
                function(N){rnorm(n=length(a),mean=a+bN*N,sd=sigma)})
K.mu <- apply(sim.K,2,mean)
K.pi <- apply(sim.K,2,PI,prob=0.89)

plot(NULL, xlim=c(-2,2), ylim=c(-2,2),
     xlab="N (std)", ylab="K (std)")
lines(x=N.seq, y=K.mu)
shade(K.pi, N.seq)
```

As we can see, the prior is very flat. We can make it stronger by reducing the standard deviation of parameters.

```{r}
set.seed(2)
N <- 100
a <- rnorm(N,0,0.2)
bN <- rnorm(N,0,0.5)
sigma <- rexp(N,1)
```


```{r}
plot(NULL, xlim=range(dcc$N),ylim=c(-2,2),
     xlab="N (std)", ylab="K (std)")
for (i in 1:50){
  curve(a[i]+bN[i]*x, from=min(dcc$N), to=max(dcc$N), add=TRUE,
        col=col.alpha(rangi2))
}
```


```{r}
N.seq <- seq(from=-2, to=2, length.out=30)
sim.K <- sapply(N.seq,
                function(N){rnorm(n=length(a),mean=a+bN*N,sd=sigma)})
K.mu <- apply(sim.K,2,mean)
K.pi <- apply(sim.K,2,PI,prob=0.89)

plot(NULL, xlim=c(-2,2), ylim=c(-2,2),
     xlab="N (std)", ylab="K (std)")
lines(x=N.seq, y=K.mu)
shade(K.pi, N.seq)
```

## Updated statistical models

$$
K \sim Normal(mu, sigma)\\
mu = \alpha + \beta_{N}N\\
\alpha \sim Normal(0,0.2)\\
\beta_{N} \sim Normal(0,0.5)\\
\sigma \sim Exponential(1)
$$
```{r}
dlist <- list(
  N = dcc$N,
  K = dcc$K,
  M = dcc$M,
  L = nrow(dcc)
)
```

```{r}
code_m1.1 <- "
data{
  int L;
  vector[L] N;
  vector[L] M;
  vector[L] K;
}
parameters{
  real a;
  real bN;
  real<lower=0> sigma;
}
model{
  vector[L] mu;
  sigma ~ exponential(1);
  a ~ normal(0,0.2);
  bN ~ normal(0,0.5);
  for(i in 1:L){
    mu[i] = a + bN*N[i];
  }
  K ~ normal(mu, sigma);
}
generated quantities{
  vector[L] log_lik;
  vector[L] mu;
  for(i in 1:L){
    mu[i] = a + bN*N[i];
  }
  for(i in 1:L){
    log_lik[i] = normal_lpdf(K[i]|mu, sigma);
  }
}
"
```


```{r}
m.1.1 <- stan(model_code=code_m1.1, data=dlist, chains=4, cores=4)
precis(m.1.1)
```

The model outcome shows no association between N and K. Let's try with another predictor variable, M

```{r}
code_m1.2 <- "
data{
  int L;
  vector[L] N;
  vector[L] M;
  vector[L] K;
}
parameters{
  real a;
  real bM;
  real<lower=0> sigma;
}
model{
  vector[L] mu;
  sigma ~ exponential(1);
  a ~ normal(0,0.2);
  bM ~ normal(0,0.5);
  for(i in 1:L){
    mu[i] = a + bM*M[i];
  }
  K ~ normal(mu, sigma);
}
generated quantities{
  vector[L] log_lik;
  vector[L] mu;
  for(i in 1:L){
    mu[i] = a + bM*M[i];
  }
  for(i in 1:L){
    log_lik[i] = normal_lpdf(K[i]|mu, sigma);
  }
}
"
```


```{r}
m.1.2 <- stan(model_code=code_m1.2, data=dlist, chains=4, cores=4)
precis(m.1.2)
```

Again, there is no significant association between M and K. One more try is to include both M and N as predictor variables for K

```{r}
code_m1.3 <- "
data{
  int L;
  vector[L] N;
  vector[L] M;
  vector[L] K;
}
parameters{
  real a;
  real bM;
  real bN;
  real<lower=0> sigma;
}
model{
  vector[L] mu;
  sigma ~ exponential(1);
  a ~ normal(0,0.2);
  bM ~ normal(0,0.5);
  bN ~ normal(0,0.5);
  for(i in 1:L){
    mu[i] = a + bM*M[i] + bN*N[i];
  }
  K ~ normal(mu, sigma);
}
generated quantities{
  vector[L] log_lik;
  vector[L] mu;
  for(i in 1:L){
    mu[i] = a + bM*M[i] + bN*N[i];
  }
  for(i in 1:L){
    log_lik[i] = normal_lpdf(K[i]|mu, sigma);
  }
}
"
```


```{r}
m.1.3 <- stan(model_code=code_m1.3, data=dlist, chains=4, cores=4)
precis(m.1.3)
```

```{r}
# plot(coeftab(m.1.1, m.1.2, m.1.3), pars=c("bM", "bN"))
```

The outcome is contrast to the previous ones as both M and N have significant association with K, but in the opposite ways. The reasonable explanation is that M and N are correlated with the outcome variables, but one is positive correlated with it and the other is negatively correlated with it.

## DAG and counterfactual plots

Given the data alone, it is hard to draw a causal diagram. Let's try with one diagram below.

```{r}
dag1.1 <- dagitty("dag{
                  U [unobserved]
                  M -> K
                  M <- U -> N
                  N -> K
}")
coordinates(dag1.1) <- list(x=c(M=0,K=1,U=1,N=2),
                            y=c(M=0,K=1,U=0,N=0))
drawdag(dag1.1)
```
Now, we can make counter-factual plots.

```{r}
post1.3 <- extract.samples(m.1.3)
N <- 0
M.seq = seq(from=min(dcc$M)-0.15, to=max(dcc$M)+0.15, length.out=30)
mu <- matrix(0,nrow=length(post1.3$a), ncol=30)
for(i in 1:30){mu[,i] = post1.3$a + post1.3$bM*M.seq[i] + post1.3$bN*N}
# mu <- link(m1.3, data=data.frame(M=M.seq, N=0))
mu.mean <- apply(mu,2,mean)
mu.pi <- apply(mu,2,PI,prob=0.89)
```

```{r}
plot(NULL, xlim=range(dcc$M), ylim=range(dcc$K),
     xlab="M (std)", ylab="K (std)")
lines(x=M.seq, y=mu.mean, lwd=2)
shade(mu.pi, M.seq)
mtext("Counterfactural holding N=0")
```

```{r}
M <- 0
N.seq = seq(from=min(dcc$N)-0.15, to=max(dcc$N)+0.15, length.out=30)
mu <- matrix(0,nrow=length(post1.3$a), ncol=30)
for(i in 1:30){mu[,i] = post1.3$a + post1.3$bM*M + post1.3$bN*N.seq[i]}


mu <- link(m1.3, data=data.frame(N=N.seq, M=0))
mu.mean <- apply(mu,2,mean)
mu.pi <- apply(mu,2,PI,prob=0.89)
```


```{r}
plot(NULL, xlim=range(dcc$N), ylim=range(dcc$K),
     xlab="N (std)", ylab="K (std)")
lines(x=N.seq, y=mu.mean, lwd=2)
shade(mu.pi, N.seq)
mtext("Counterfactural holding N=0")
```

# 2.Imputation

In the original data set, there are 12 missing values in N. Previously, dropping such rows with missing values left us with only 17 cases to work with. In this section, we will try to impute these missing values and extract information from them.

## DAGs

We are going to draw a DAG that contains our assumption about the cause of missing value of N. The simple DAG below implies of a missing completely at random as nothing influences Rb.

```{r}
dag1.2 <- dagitty("dag{
                  B [unobserved]
                  U [unobserved]
                  M->K<-B
                  M<-U->B
                  Rb->B_obs<-B
}")
coordinates(dag1.2) <- list(x=c(M=0,K=1,U=1,Rb=1,B=2,B_obs=2),
                            y=c(M=0,K=1,U=0,Rb=-1,B=0,B_obs=-1))
drawdag(dag1.2)
```

## Statistical models

$$
K_{i} \sim Normal(\mu_{i},\sigma)\\
\mu_{i} = \alpha + \beta_{N}N_{i} + \beta_{M}M_{i}\\
N_{i} \sim Normal(\nu, \sigma_{N})\\
\alpha \sim Normal(0,0.5)\\
\beta_{N} \sim Normal(0,0.5)\\
\beta_{M} \sim Normal(0,0.5)\\
\sigma \sim Exponential(1)\\
\nu \sim Normal(0.5,1)\\
\sigma_{N} \sim Exponential(1)
$$
## Prior predictive checks

```{r}
set.seed(3)
N <- 100
a <- rnorm(N,0,0.5)
bN <- rnorm(N,0,0.5)
bM <- rnorm(N,0,0.5)
nu <- rnorm(N,0.5,1)
sigmaN <- rexp(N,1)
N.var <- rnorm(N,nu,sigmaN)
```

```{r}
plot(NULL, xlim=range(M), ylim=c(-2,2),
     xlab="M (std)", ylab="K (std)")
for(i in 1:50){
  curve(a[i] + bN[i]*N.var[i] + bM[i]*x, from=min(dcc$M), to=max(dcc$M), add=TRUE,col=rangi2)
}
```

## Fit the model

```{r}
d$neocortex.prop <- d$neocortex.perc/100
dlist2 <- list(
  N = ifelse(is.na(standardize(d$neocortex.prop)),-1,standardize(d$neocortex.prop)),
  K = d$K,
  M = d$M,
  L = nrow(d),
  L_miss = sum(apply(d, 1, anyNA)),
  N_missidx = unique(which(is.na(d), arr.ind=TRUE)[,1])
)
```


```{r}
code_m2.1 <- "
functions{
  vector merge_missing(int[] miss_indexes, vector x_obs, vector x_miss){
    int N = dims(x_obs)[1];
    int N_miss = dims(x_miss)[1];
    vector[N] merged;
    merged = x_obs;
    for(i in 1:N_miss) merged[miss_indexes[i]] = x_miss[i];
    return merged;
  }
}
data{
  int L;
  int L_miss;
  vector[L] N;
  vector[L] K;
  vector[L] M;
  int N_missidx[L_miss];
}
parameters{
  real a;
  real bN;
  real bM;
  real nu;
  real<lower=0> sigma;
  real<lower=0> sigma_N;
  vector[L_miss] N_impute;
}
model{
  vector[L] mu;
  vector[L] N_merge;
  sigma_N ~ exponential(1);
  sigma ~ exponential(1);
  bN ~ normal(0,0.5);
  bM ~ normal(0,0.5);
  a ~ normal(0,0.5);
  nu ~ normal(0,0.5);
  
  N_merge = merge_missing(N_missidx, to_vector(N), N_impute);
  N_merge ~ normal(nu, sigma_N);
  
  for(i in 1:L){
    mu[i] = a + bN*N_merge[i] + bM*M[i];
  }
  K ~ normal(mu, sigma);
}
generated quantities{
  vector[L] mu;
  vector[L] log_lik;
  vector[L] N_merge;
  
  N_merge = merge_missing(N_missidx, to_vector(N), N_impute);
  
  for(i in 1:L){
    mu[i] = a + bN*N_merge[i] + bM*M[i];
  }
  for(i in 1:L){
    log_lik[i] = normal_lpdf(K[i] | mu[i], sigma);
  }
}
"
```


```{r}
m.2.1 <- stan(model_code=code_m2.1, data=dlist2, chains=4, cores=4)
precis(m.2.1,2)
```

## Posterior predictive check

```{r}
post <- extract.samples(m.2.1)
N.impute.mu <- apply(post$N_impute,2,mean)
N.impute.pi <- apply(post$N_impute, 2, PI, prob=0.89)
```


```{r}
plot(NULL, xlim=c(-2,2), ylim=range(dlist2$K),
     col=rangi2, xlab="N (std)", ylab="K (std)")
points(x=d$N, y=d$K, col=rangi2, pch=16)
miss.idx <- unique(which(is.na(d), arr.ind=TRUE)[,1])
Ki <- dlist2$K[miss.idx]
points(N.impute.mu, Ki)
for(i in 1:12){
  lines(x=N.impute.pi[,i],y=rep(Ki[i],2))
}
```


```{r}
plot(NULL, xlim=c(-2,2), ylim=c(-2,2),
     xlab="M (std)", ylab="N (std)")
points(x=d$M, y=d$N, col=rangi2, pch=16)
miss.idx <- unique(which(is.na(d), arr.ind=TRUE)[,1])
Mi <- dlist2$M[miss.idx]
points(y=N.impute.mu,x=Mi)
for(i in 1:12){
  lines(x=rep(Mi[i],2), y=N.impute.pi[,i])
}
```

One thing we learn from these two charts is that the imputed values do not fit well into the variables' association caused by observed data points. We expect to improve the imputation by encoding the information of relationship between M and N into the imputing process.

## Updated statistical model

$$
K_{i} \sim Normal(\mu_{i},\sigma)\\
\mu_{i} = \alpha + \beta_{N}N_{i} + \beta_{M}M_{i}\\
N_{i} \sim Normal(\nu, \sigma_{N})\\

\left( \begin{array}{c}
M_{i}\\
N_{i}\end{array} \right) \sim MVNormal(\left( \begin{array}{c}
\mu_{M}\\\mu_{N}\end{array} \right),S)\\
R \sim LKJcorr(2)\\
\alpha \sim Normal(0,0.5)\\
\beta_{N} \sim Normal(0,0.5)\\
\beta_{M} \sim Normal(0,0.5)\\
\sigma \sim Exponential(1)\\
\nu \sim Normal(0.5,1)\\
\sigma_{N} \sim Exponential(1)
$$

```{r}
code_m2.2 <- "
functions{
  vector merge_missing(int[] miss_indexes, vector x_obs, vector x_miss){
    int N = dims(x_obs)[1];
    int N_miss = dims(x_miss)[1];
    vector[N] merged;
    merged = x_obs;
    for(i in 1:N_miss) merged[miss_indexes[i]] = x_miss[i];
    return merged;
  }
}
data{
  int L;
  int L_miss;
  vector[L] N;
  vector[L] K;
  vector[L] M;
  int N_missidx[L_miss];
}
parameters{
  real muM;
  real muN;
  real a;
  real bN;
  real bM;
  real<lower=0> sigma;
  corr_matrix[2] Rho_NM;
  vector<lower=0>[2] Sigma_NM;
  vector[L_miss] N_impute;
}
model{
  vector[L] mu;
  matrix[L,2] MN;
  vector[L] N_merge;
  Sigma_NM ~ exponential(1);
  Rho_NM ~ lkj_corr(2);
  sigma ~ exponential(1);
  bN ~ normal(0,0.5);
  bM ~ normal(0,0.5);
  a ~ normal(0,0.5);
  muM ~ normal(0,0.5);
  muN ~ normal(0,0.5);
  
  N_merge = merge_missing(N_missidx, to_vector(N), N_impute);
  MN = append_col(M, N_merge);
  {
  vector[2] MU;
  MU = [muM, muN]';
  for(i in 1:L) MN[i,:] ~ multi_normal(MU, quad_form_diag(Rho_NM, Sigma_NM));
  }
  for(i in 1:L){
    mu[i] = a + bN*N_merge[i] + bM*M[i];
  }
  K ~ normal(mu, sigma);
}
generated quantities{
  vector[L] mu;
  vector[L] log_lik;
  vector[L] N_merge;
  matrix[L,2] MN;
  
  N_merge = merge_missing(N_missidx, to_vector(N), N_impute);
  MN = append_col(M, N_merge);
  for(i in 1:L){
    mu[i] = a + bN*N_merge[i] + bM*M[i];
  }
  for(i in 1:L){
    log_lik[i] = normal_lpdf(K[i] | mu[i], sigma);
  }
}
"
```

```{r}
m.2.2 <- stan(model_code=code_m2.2, data=dlist2, chains=4, cores=4)
precis(m.2.2)
```

```{r}
post <- extract.samples(m.2.2)
N_impute_mu <- apply(post$N_impute, 2, mean)
N_impute_ci <- apply(post$N_impute, 2, PI)
```


```{r}
plot(d$N, d$K, pch=16, col=rangi2,
     xlab="neucortex percent (std)",
     ylab="kcal milk (std)")

miss_idx <- unique(which(is.na(d), arr.ind=TRUE)[,1])
Ki <- d$K[miss_idx]
points(N_impute_mu, Ki)

for(i in 1:12){
  lines(N_impute_ci[,i], rep(Ki[i],2))
}
```

```{r}
plot(d$M, d$N, pch=16, col=rangi2,
     ylab="neucortex percent (std)",
     xlab="log body mass (std)")

Mi <- d$M[miss_idx]
points(Mi, N_impute_mu)
for(i in 1:12){
  lines(rep(Mi[i],2), N_impute_ci[,i])
}
```
That is what we expected.

#3. Categorical variables

We are going to analyze the difference in K between four groups covered by the clade variable

```{r}
d$clade_id <- as.integer(d$clade)
```

## Statistical model

$$
K_{i} \sim Normal(\mu_{i},\sigma)\\
\mu_{i} = \alpha_{clade[i]}\\
\alpha_{j} \sim Normal(0,0.5)\\
\sigma \sim Exponential(1)
$$

```{r}
dlist3 <- list(
  K = d$K,
  clade_id = d$clade_id,
  N = nrow(d),
  id = length(unique(d$clade_id))
)
```

```{r}
code_m3.1 <- "
data{
  int N;
  int id;
  vector[N] K;
  int clade_id[N];
}
parameters{
  vector[id] a;
  real<lower=0> sigma;
}

model{
  vector[N] mu;
  sigma ~ exponential(1);
  a ~ normal(0,0.5);
  for(i in 1:N){
    mu[i] = a[clade_id[i]];
  }
  K ~ normal(mu, sigma);
}
generated quantities{
  vector[N] log_lik;
  vector[N] mu;
  for(i in 1:N){
    mu[i] = a[clade_id[i]];
  }
  for(i in 1:N) log_lik[i] = normal_lpdf(K[i] | mu[i], sigma);
}
"
```

```{r}
m.3.1 <- stan(model_code=code_m3.1, data=dlist3, chains=4, cores=4)
precis(m.3.1,2)
```


```{r}
labels <- paste( "a[" , 1:4 , "]:" , levels(d$clade) , sep="" )
plot( precis( m.3.1 , depth=2 , pars="a" ) , labels=labels ,
    xlab="expected kcal (std)" )
```


```{r}
```
